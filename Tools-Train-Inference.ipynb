{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2778fde9-ad4b-4ece-8af8-3cc75603dc13",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8845f-c37a-4219-acb9-2120b78a586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\n",
    "python -m torch.distributed.launch --nproc_per_node=8 \\\n",
    "/root/ACCV2022/train.py \\\n",
    "--csv-dir autodl-tmp/ACCV_384_balance_fold.csv \\\n",
    "--config-name 'timm' \\\n",
    "--image-size 384 \\\n",
    "--batch-size 7 \\\n",
    "--num-workers 10 \\\n",
    "--init-lr 6e-5 \\\n",
    "--n-epochs 10 \\\n",
    "--cpkt_epoch 1 \\\n",
    "--n_batch_log 300 \\\n",
    "--warm_up_epochs 1 \\\n",
    "--fold 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b77db-0dab-4825-b3de-c07a6bafd58a",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a50af-169f-423b-83bc-7eab18d79add",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ACCV2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7a988-5d90-44d1-a3c9-326e63b37b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from build_model import XL_TIMM_Net, XL_DEIT_Net\n",
    "from config_timm_deit_arc import get_config\n",
    "config_deit_arc = get_config()\n",
    "from config_timm_beit_384_sub import get_config\n",
    "config_beit_sub = get_config()\n",
    "from config_timm_beitv2_sub import get_config\n",
    "config_beitv2_sub = get_config()\n",
    "from config_timm_swin_sub import get_config\n",
    "config_swin_sub = get_config()\n",
    "from config_timm_beit_512_sub import get_config\n",
    "config_beit512_sub = get_config()\n",
    "from config_timm_swinv2_sub import get_config\n",
    "config_swinv2_sub = get_config()\n",
    "from config_timm_Deit_sub import get_config\n",
    "config_Deit_sub = get_config()\n",
    "from dataset import ACCV_test\n",
    "\n",
    "model1 = XL_TIMM_Net(config_beit_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/beit_new_wash_727_384.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model1.load_state_dict(new_state_dict, strict=True)\n",
    "model1.cuda().eval()\n",
    "\n",
    "model2 = XL_TIMM_Net(config_beit_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/TIMM_best_731_beit_384.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model2.load_state_dict(new_state_dict, strict=True)\n",
    "model2.cuda().eval()\n",
    "\n",
    "model3 = XL_TIMM_Net(config_beit_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/TIMM_best_730_beit_384.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model3.load_state_dict(new_state_dict, strict=True)\n",
    "model3.cuda().eval()\n",
    "\n",
    "model4 = XL_TIMM_Net(config_beit512_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/beit_512_ep3.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model4.load_state_dict(new_state_dict, strict=True)\n",
    "model4.cuda().eval()\n",
    "\n",
    "model5 = XL_TIMM_Net(config_swinv2_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/swinv2_384_epoch_2.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model5.load_state_dict(new_state_dict, strict=True)\n",
    "model5.cuda().eval()\n",
    "\n",
    "model6 = XL_TIMM_Net(config_deit_arc, None)\n",
    "state_dict = torch.load('../autodl-tmp/deit_704_384_arc.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model6.load_state_dict(new_state_dict, strict=True)\n",
    "model6.cuda().eval()\n",
    "\n",
    "model7 = XL_TIMM_Net(config_beitv2_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/TIMM_beitv2_224.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model7.load_state_dict(new_state_dict, strict=True)\n",
    "model7.cuda().eval()\n",
    "\n",
    "model8 = XL_TIMM_Net(config_swin_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/swin_384_sub_epoch_3.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model8.load_state_dict(new_state_dict, strict=True)\n",
    "model8.cuda().eval()\n",
    "\n",
    "model9 = XL_DEIT_Net(config_Deit_sub, None)\n",
    "state_dict = torch.load('../autodl-tmp/deit_384_cls_sub_epoch_3.pth', map_location='cpu')['state_dict']\n",
    "new_state_dict = {}\n",
    "for k,v in state_dict.items():\n",
    "    key = k.replace('module.','')\n",
    "    new_state_dict[key]=v\n",
    "model9.load_state_dict(new_state_dict, strict=True)\n",
    "model9.cuda().eval()\n",
    "print('>>>>>>>>>> Load Successful! <<<<<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bc7c0-4588-4ec2-afbd-a877361a50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from torchvision import transforms\n",
    "test_files = glob('../autodl-tmp/testb/*')\n",
    "df_dict = {'id': test_files}\n",
    "df = pd.DataFrame(df_dict)\n",
    "\n",
    "dataset_test = ACCV_test(df, 384)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=64, num_workers=16, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "pred_cls = []\n",
    "\n",
    "test_files = glob('../autodl-tmp/testb/*')\n",
    "\n",
    "for imgs in tqdm(dataloader_test):\n",
    "    with torch.no_grad():\n",
    "        imgs = imgs.cuda()\n",
    "        imgs1 = transforms.functional.resize(imgs, size=[600, 600])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[384, 384])\n",
    "        pred = 0\n",
    "        pred += model1(imgs1).softmax(1)\n",
    "        pred += model2(imgs1).softmax(1)\n",
    "        pred += model3(imgs1).softmax(1)\n",
    "        pred += 0.8*model9(imgs1).softmax(1)\n",
    "        pred += 0.6*model5(imgs1).softmax(1)\n",
    "        pred += 0.6*model6(imgs1).softmax(1)\n",
    "        pred += 0.6*model8(imgs1).softmax(1)\n",
    "    \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[480, 480])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[384, 384])\n",
    "        pred += 0.3*model1(imgs1).softmax(1)\n",
    "        pred += 0.3*model2(imgs1).softmax(1)\n",
    "        pred += 0.3*model3(imgs1).softmax(1)\n",
    "        pred += 0.24*model9(imgs1).softmax(1)\n",
    "        pred += 0.18*model5(imgs1).softmax(1)\n",
    "        pred += 0.18*model6(imgs1).softmax(1)\n",
    "        pred += 0.18*model8(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[540, 540])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[384, 384])\n",
    "        pred += 0.4*model1(imgs1).softmax(1)\n",
    "        pred += 0.4*model2(imgs1).softmax(1)\n",
    "        pred += 0.4*model3(imgs1).softmax(1)\n",
    "        pred += 0.32*model9(imgs1).softmax(1)\n",
    "        pred += 0.24*model5(imgs1).softmax(1)\n",
    "        pred += 0.24*model6(imgs1).softmax(1)\n",
    "        pred += 0.24*model8(imgs1).softmax(1)\n",
    "    \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[660, 660])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[384, 384])\n",
    "        pred += 0.4*model1(imgs1).softmax(1)\n",
    "        pred += 0.4*model2(imgs1).softmax(1)\n",
    "        pred += 0.4*model3(imgs1).softmax(1)\n",
    "        pred += 0.32*model9(imgs1).softmax(1)\n",
    "        pred += 0.24*model5(imgs1).softmax(1)\n",
    "        pred += 0.24*model6(imgs1).softmax(1)\n",
    "        pred += 0.24*model8(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[700, 700])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[384, 384])\n",
    "        pred += 0.3*model1(imgs1).softmax(1)\n",
    "        pred += 0.3*model2(imgs1).softmax(1)\n",
    "        pred += 0.3*model3(imgs1).softmax(1)\n",
    "        pred += 0.24*model9(imgs1).softmax(1)\n",
    "        pred += 0.18*model5(imgs1).softmax(1)\n",
    "        pred += 0.18*model6(imgs1).softmax(1)\n",
    "        pred += 0.18*model8(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[400, 400])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[224, 224])\n",
    "        pred += 0.3*model7(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[370, 370])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[224, 224])\n",
    "        pred += 0.4*model7(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[350, 350])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[224, 224])\n",
    "        pred += model7(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[330, 330])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[224, 224])\n",
    "        pred += 0.4*model7(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[300, 300])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[224, 224])\n",
    "        pred += 0.3*model7(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[700, 700])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[512, 512])\n",
    "        pred += 0.5*model4(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[750, 750])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[512, 512])\n",
    "        pred += 0.6*model4(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[800, 800])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[512, 512])\n",
    "        pred += 1.4*model4(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[850, 850])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[512, 512])\n",
    "        pred += 0.6*model4(imgs1).softmax(1)\n",
    "        \n",
    "        imgs1 = transforms.functional.resize(imgs, size=[900, 900])\n",
    "        imgs1 = transforms.functional.center_crop(imgs1, output_size=[512, 512])\n",
    "        pred += 0.5*model4(imgs1).softmax(1)\n",
    "        \n",
    "        pred_cls.extend(pred.argmax(1).cpu().detach().numpy())\n",
    "\n",
    "for i in trange(len(test_files)):\n",
    "    test_files[i] = test_files[i].split('/')[-1]\n",
    "    pred_cls[i] = str(pred_cls[i]).zfill(4)\n",
    "\n",
    "df_dict = {'id': test_files,\n",
    "           'pred': pred_cls}\n",
    "\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('../autodl-tmp/pred_results.csv', index=False, header=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
